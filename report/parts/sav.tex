\subsection{Разработка метода поиска наилучшей плоскости для проекции (Савосин Артем)}

На вход подаются пред обработанные данные - отфильтрованные, усредненные по тактам (если их было несколько). Данные задаются массивом из 3-х мерных векторов, которые выглядят вот так
\begin{figure}[H]
\center{\includegraphics[scale = 0.5]{images_sav/3d_fig.png}}
\end{figure}
Для наших данных хотим найти такую двумерную плоскость, что на ней отобразится максимум информации про наше движение - минимум точек совпадут с друг с другом и 2d картинка останется максимально похожей на свой 3d аналог. 

Уравнение плоскости задается уравнением $ax+by+c=z$. 
Задаем систему уравнений 
\begin{figure}[H]
\center{\includegraphics[scale = 1]{images_sav/math1.png}}
\end{figure}
где $(x_i, y_i, z_i)$  - это векторы из массива, а (a, b, c) - нормаль, которую нам нужно найти. В силу того, что в массиве у нас явно больше, чем 3 вектора, система будет переполненной, и нам нужно будет использовать псевдообратную матрицу 
\begin{figure}[H]
\center{\includegraphics[scale = 1]{images_sav/math2.png}}
\end{figure}
Таким образом нормаль плоскости находится по формуле
\begin{figure}[H]
\center{\includegraphics[scale = 1]{images_sav/math3.png}}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[scale = 1]{images_sav/flat_plot.png}}
\end{figure}
Таким образом мы нашли нормаль для подходящей плоскости. Теперь остается только стандартным способом найти проекцию всех точек на эту плоскость и получить плоскую картинку движения.
\begin{figure}[H]
\center{\includegraphics[scale = 1]{images_sav/2d_fig.png}}
\end{figure}
Получилась плоская фигура, находящаяся в 3D пространстве, из нее получаем 2D изображение.
\begin{figure}[H]
\center{\includegraphics[scale = 1]{images_sav/2d_fig_img.png}}
\end{figure}
\subsection{Классификация данных (Савосин Артем, Макаров Максим)}

После проделанной выше работы - получение, фильтрация, выделение такта, нахождение среднего движения из нескольких тактов, перенос 3-х мерных данных в 2-х мерное изображение, остается только классифицировать полученный набор данных. 
\begin{figure}[H]
\center{\includegraphics[scale = 1]{images_sav/data_examples.png}}
\end{figure}
Выше представлены примеры данных, полученных в ходе работы. Каждый ряд картинок отображает один из классов движений - круг, квадрат, встряска. Человеческий мозг легко бы справился с задачей распознать и классифицировать новое изображение, основываясь даже на таком небольшом количестве примеров. Легко сказать, что у крогов нет углов, квадрат состоит из 4 углов, а встряска выглядит как прямая линия. Эта задача для человека кажется очень простой, но реализовать ее программно - более сложный и трудоемкий процесс. Нашей команде понадобилось бы большое количество времени, чтобы описать все условия принадлежности той или иной фигуры к определенному классу, если бы нам вообще удалось это сделать. Решить задачу классификации мы сможем, применив сверточную нейронную сеть.
\subsubsection{Что такое нейронная сеть?}
Это математическая модель, изобретенная в ходе изучения работы мозга и попытке смоделировать его поведение. 
Мозг человека состоит из нейронов, соединенных между собой каналами, по которым передается электрический импульс. Нейроны получают сигналы от многих других нейронов, обрабатывают их, и посылают новый сигнал. 
Когда мы видим какое-то изображение, в зрительной коре головного мозга активируются строго определенные участки, которые передают сигнал далее, где он обрабатывается, и мы понимаем, что перед нами, скажем, круг, а не квадрат.
Компьютерная нейронная сеть - это попытка хотя бы частично воссоздать процесс, происходящий в голове в момент распознавания картинки.  Простая нейронная сеть состоит из нескольких слоёв. 
\begin{figure}[H]
\center{\includegraphics[scale = 1]{images_sav/neuro.png}}
\end{figure}

Входной слой (зеленый) - это “глаза” нашей сети.
 Скрытый слой (синий)
Выходной слой (желтый) - в этом слое ровно столько нейронов, сколько мы имеем классов изображений. А сила сигнала на том или ином нейроне говорит о том, насколько сильно входной объект похож на объект, за который отвечает этот нейрон.
\subsubsection{Что такое сверточная нейронная сеть?}
СНС состоит из разных видов слоев: сверточные (convolutional) слои, субдискретизирующие (subsampling, подвыборка) слои и слои «обычной» нейронной сети – персептрона.
Главное отличие такой сети - наличие сверточных слоев. Перед тем, как тренировать модель, изображение будет пропущено через ряд сверток - некоторое количество фильтров, позволяющих выделить характерные признаки изображения.
\subsubsection{Так как же сеть классифицирует изображения?}
Как я уже отмечал ранее, нейроны связаны между собой каналами. 
Входные нейроны связываются со скрытыми, скрытые с выходными. На каждой такой связи расставляются веса - насколько сильно важен для следующего нейрона входящий в него по этой связи нейрон. Далее каждый нейрон производит некоторые вычисления внутри себя, и передает сигнал далее.
Когда на вход подается изображение, входные нейроны задействуются, а сила сигнала, передаваемого ими, зависит от того, насколько яркая картинка в этом месте. Сигнал передается далее, происходят вычисления во внутренних слоях. После этого на каких-то выходных нейронах появится сигнал. Мы понимаем, какие выходные нейроны активны, и насколько они активны, и делаем вывод о принадлежности изображения к тому или иному классу.
\subsubsection{Тренировка нейронной сети.}
Нужно отметить, что при создании сети, веса между нейронами расставляются случайно - они никак не связаны с изображениями, которые мы хотим классифицировать. Далее мы прогоняем через такую сеть наши данные, и выявляем ошибки. На основе этих ошибок меняются веса связей, и мы снова прогоняем наши данные и смотрим, насколько хорошо классифицируется. Потом еще раз вносим коррективы в веса связей, и так далее. На каждом шаге обучения веса меняются с помощью функции оптимизации.


\input{parts/max_ml}

\subsubsection{Модель}

В силу того, что описаная выше модель работает через web - интерфейс, а в нашем проекте требуется реализовать бибилотеку, которую можно развернуть у себя на устройстве, нам пришлось находить новое решение для классификации данных.
Как уже было описано ранее, в нашем проекте решено было использовать библиотеку tensorflow, имеющую обширное количество готовых решений, в том числе и для нейронной сети, которую можно запустить у себя на устройстве.

Для начала про данные - входные изображения разделены в пропорции 2 к 1 на тренировочные и валидационные. На первых модель обучается, вторые нужны для проверки на переобученность модели. 
Используя библиотеку keras preprocessing, мы нормируем все изображения так, чтобы входная матрица принимала цветовые значения не от 0 до 255, а от 0 до 1. Это требуется по причине того, что сети удобнее работать со значениями от 0 до 1. Далее в тренировочные данные добавляются измененные версии тренировочных файлов - изображения, повернутые на 45 градусов, Этот шаг нужен для того, чтобы избежать переобучения модели - ведь движения, записанные новым пользователем могут отличаться от собранных. 
Далее поговорим о самой модели и слоях, из которых она состоит
\begin{figure}[H]
\center{\includegraphics[scale = 1]{images_sav/model.png}}
\end{figure}
Входной слой состоит сверточного слоя ,который будет генерировать 32 фильтра для выделения параметров, за которым следует слой MaxPooling, который выделяет один, самый сильный сигнал из группы 3 на 3 и оставляет его, что позволяет значительно уменьшить изображение, сохранив его главные детали.  Далее повторим процесс, добавив новый сверточный слоев с большим количеством фильтров, таким образом, разбирая наше изображение на большое количество маленьких параметров, из которых складывается представление о фигуре.
Далее следуют слои обычной нейронной сети, с еще 516 фильтрами.
Последний слой состоит из 3 нейронов (по количеству классов) и активатора softmax. Активатор выбирает максимальное значение из трех нейронов, и присваивает ему весь сигнал, остальные нейроны будут пустыми. 

Про точность модели 
на финальном шаге обучения точность модели на обучаемых данных составила 97,92 процентов , а на независимых данных модель смогла угадать все 100 процентов представленных изображений. 
За два шага до этого модель укладывала 100 процентов всех тренировочных данных. \newline
Epoch 18/20 \newline
accuracy: 1.0000 - val-accuracy: 1.0000 \newline
Epoch 19/20\newline
accuracy: 1.0000 - val-accuracy: 0.9792\newline
Epoch 20/20\newline
accuracy: 1.0000 - val-accuracy: 0.9792\newline
\begin{figure}[H]
\center{\includegraphics[scale = 1]{images_sav/accuracy.png}}
\end{figure}
\subsubsection{Описание вычислительного эксперимента.}

Для определения наиболее подходящей классифицирующей модели для наших данных, было решено провести ряд экспериментов.

Первым экспериментом был замер точности простой модели, состоящей из 2 сверточных слоев, без каких бы то ни было поправок на переобучение.

\begin{figure}[H]
\center{\includegraphics[scale = 1]{images_sav/model1.png}}
\end{figure}

Для данной модели были выбраны 4 функции оптимизации:

\begin{itemize}
	\item Оптимизация Адама - это метод стохастического градиентного спуска, основанный на адаптивной оценке моментов первого и второго порядка.
	\item Adagrad - это оптимизатор с частотой обучения, зависящей от параметров, которая адаптируется в зависимости от того, как часто параметр обновляется во время обучения. Чем больше обновлений получает параметр, тем меньше обновлений.
	\item SGD - Стохастический градиентный спуск и оптимизатор импульса.
	\item Rmsprop
\end{itemize}

Данные генерировались с помощью простой функции, которая переводила матрицу со значениями от 0 до 255 в значения от 0 до 1.

Результаты работы предоставлены на графиках ниже.

\begin{figure}[H]
\center{\includegraphics[scale = 0.8]{images_sav/exp1.png}}
\end{figure}

Заметим, что все модели показывают довольно неплохую точность - распознавание тренировочных данных не ниже 99 процентов, валидационных - 95. Главное отличие в скорости достижения высокой точности и потерях нейронов в ходе обучения. Лучше всего себя показала модель с функцией оптимизации adam.

Обычно, при обучении нейронной сети главной проблемой является переобучение, в результате которого модель начинает очень хорошо распознавать изображения, на которых она обучалась, и гораздо хуже распознает новые изображения. Проверим, что будет с нашими моделями, если мы добавим в них методы, противодействующие переобучению. 

Добавим еще один сверточный слой и введем новые слои Dropout, которые выкидывают 20 процентов обученных нейронов на каждом шаге, что уменьшает риск переобучения.  
\begin{figure}[H]
\center{\includegraphics[scale = 1]{images_sav/model2.png}}
\end{figure}
Далее проверяем точность этих моделей на тех же 4 функциях.

\begin{figure}[H]
\center{\includegraphics[scale = 0.8]{images_sav/exp2.png}}
\end{figure}

Заметим, что графики стали более волатильными - возросло количество потерь, упала точность, теперь тренировочная точность лежит в пределах от 93 до 100, а валидационная - от 87 до 100. Лучше всех себя опять показала модель с оптимизатором adam. 

Добавим к данной модели еще несколько . Теперь тренировочные данные будут разбавляться набор из видоизмененных жестов - растянутых, повернутых на 45 градусов, смещенных относительно центра.
В силу того, что данных получится несколько больше, увеличим количество тренировочных шагов.
Прроверим эти нововведения на двух оптимизаторах - самом точном и стабильном на данный момент (adam) и самый волатильный (rmsprop)

\begin{figure}[H]
\center{\includegraphics[scale = 0.5]{images_sav/exp3.png}}
\end{figure}

На первых двух графиках показана точность работы модели с оптимизатором adam, а на вторых - c rmsprop. Заметим, что точность моделей очень сильно упала, особенно у второго классификатора. 

Исходя из проделанных экспериментов можно сделать следующий вывод - наши данные имеют очень своеборазную форму, которая довольно слабо отличается внутри класса, и ввод функций, изменяющих исходные данные, сильно искажает общие черты для класса, настолько, что становится гораздо сложнее их выделять.
Исходя из результатов экспериментов было решено ввести самый первый вариант модели с самой точной функцией оптимизатором.
